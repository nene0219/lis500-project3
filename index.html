<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LIS 500 - Code and Power</title>
    <link rel="stylesheet" href="stylesheet.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
</head>

<body>

<!--
    This is left intentionally blank, the navbar gets loaded from the navbar.html file
    by the page.js script
-->
<header class="header">
</header>

<main class="main-content">
    <section class="hero">
        <h1>Project # 3 Teachable Machines</h1>
        <p>
           Our project brings AI-driven image recognition to improve recycling practices by classifying various recyclable items.
        </p>
        <p><a href="tmachines.html" class="button">Start Exploring</a></p>
    </section>

    <div class="resource-card">
        <h2>Project Statement</h2>
        <p>
            For project 3, we set out to create an image classification tool using AI to identify recyclable items from across the world. Using Teachable Machines, a tool developed by Google to assist in providing an approachable way to teach the fundamentals of AI classification. On reflecting on Buolamwini’s book, Unmasking AI, it’s interesting to navigate a world in which companies like Google are creating these helpful teaching tools while placing an excess of trust in big data that earns them her criticism. Her ethics and perspective also served as a guiding motivator for this project, since we were very aware of how code is not neutral. We understood that all technical systems are socio-technical systems, that socio-technical systems are not neutral sources of information and serve political agendas. So, we were hyperaware of how we should be teaching the machine to sort something beneficial.
<h4>Coded Gaze</h4> 
        <p> <!-- Add Ruha Benjamin citation, race after tech and intersectionality-->
            One of the core ideas from Buolamwini’s work that especially shaped our thinking was her concept of the “coded gaze”, which highlights how technology often inherits biases from those who create it. As we built our recycling classifier, we became increasingly aware of the potential for our own unconscious biases to be embedded into our algorithm. Understanding the coded gaze encouraged us to regularly question our data selection and labeling processes to ensure we weren't unintentionally privileging certain geographies or cultural contexts over others.
    </p> <p>
            With that in mind, we also looked at how recycling technologies are actually used in different places.  While AI-driven recycling bins have started appearing in various locations, however, these bins are usually found in  major cities with advanced infrastructure, limiting access for many communities. To make this type of technology more accessible, we went with a web-based tool that utilizes commonly available webcams. This approach allows a wider range of users, regardless of their location, to take advantage of improved recycling accuracy. Our goal was influenced by Buolamwini's emphasis on making technology inclusive and reducing technological gaps. Through this project, we aim to enable communities everywhere to participate equally in better waste management practices.
    </p>  <p>
            This focus on inclusivity also reminded us of a moment in the book when Buolamwini met with a delegation from Rwanda during her trip to Brussels, when she stated how her facial recognition bias research could help with the over policing of African Americans, and garnered little sympathy (Buolamwini, 176). She then illustrated that these issues in facial recognition bias may show up differently depending on global context, to which she was told that she should be building new technology instead of critiquing what is not working. Our takeaway was that thoughtful technology production and reflection of potential harms and cross-cultural contexts will only create less problems for us in the future. This sentiment drove many decisions during this project.
    </p> 
        <p>
            While building the tool, we also spent time thinking about how it might be used in different parts of the world. Our goal was to improve recycling accuracy, but we realized that if we weren’t careful, the model could end up working better for items more common in wealthier countries and miss others more typical in different regions. Being aware of this helped us focus more on balance and fairness in the way we collected data and trained the model.
    </p>   <p>
            The image data collected for this project contained images from Australia, Asia, Africa, Europe, and North and South America. The data set was primarily compromised of common recyclables, with the main differences being in the brand featured. This is where a global data set was advisable, since it wouldn’t be as accurate if we only inserted recyclables from Europe and North America. Throughout the development of the tool, we saw firsthand how different glass bottles and other similar items varied across the world. To achieve our goal of a diverse and even range of images, we aimed to start with 4 images per continent per 4 types of recyclable materials, with more to come. However, we must always analyze and reevaluate how effective our training data set is, as taking the time to reassess was an insight we gained from Buolamwini’s writings. The labels used to classify images can be an unintended means to insert bias, as where do we draw the line between paper and cardboard? In order to improve the accuracy of our model and reduce misclassification, we grouped the two together.
    </p>  <h4>Teachable Machines and Data Collection</h4> 
        <p>
            As we were working with Teachable Machines, we found the platform fairly easy to use, which allowed us to focus more on our data and testing.  Initially, we aimed for perfect accuracy, but this led to overfitting, causing the model to perform poorly on new images. Additionally, Teachable Machines auto-cropped training images, sometimes confusing classification categories, for instance, Coca Cola plastic bottles and Coca Cola cans appeared nearly identical after cropping. We also noticed the model's accuracy varied across the day due to lighting issues, leading us to include images under various lighting conditions. These experiences underscored the importance of well-balanced, representative datasets, echoing Buolamwini’s emphasis on accuracy and fairness.
    </p>   <p>
            Buolamwini's insights into AI ethics made us particularly mindful of transparency and accountability throughout the project. She emphasizes developers' responsibility to clarify how algorithms make decisions, especially when impacting diverse populations. We documented our data sources and classification decisions transparently, openly acknowledging limitations and potential biases inherent in machine learning models.
    </p>   <p>
            Toward the end of the project, we were still thinking about how larger systems of inequality show up in small ways. Buolamwini’s intersectional approach to her work also motivated ours, as global waste is perpetuated the most by Western countries, whose trash often gets discarded in Africa. This is a global issue that warrants a global data set, so we strived for even representation of recyclable items from all over. There is a gap in consumer recycling knowledge, with people often throwing away items like greasy pizza boxes that are unable to be recycled. Yet, there are certainly limitations, especially with how local recycling rules can vary depending on the facilities nearby, as well as anything else we may not have accounted for.
        </p>
    </div>

    <!--
    <section class="tiles" id="tiles">
        <a class="tile" href="tmachines.html">
            <h3>Resources</h3>
            <p>
                Access articles, books, and tools to deepen your understanding of code and power.
            </p>
        </a>
        <a class="tile" href="about_us.html">
            <h3>Meet The Team</h3>
            <p>
                Learn why we're passionate about this exploration.
            </p>
        </a>
        <a class="tile" href="tmachines.html">
            <h3>Bias Analysis</h3>
            <p>
                Uncover in-depth analyses of biases within algorithms and technological systems.
            </p>
        </a>
        <a class="tile" href="techhero.html">
            <h3>Tech Heroes</h3>
            <p>
                Learn more about the two tech heroes we've chosen to write about.
            </p>
        </a>

    </section>
    -->
    <img src="PieterHugo.png" alt="Pieter Hugo Photograph" width="580" height="562">
    <p>
        A photograph by Pieter Hugo depicting an e-waste dumping ground in Agbogbloshie, a slum in Accra, the  capital of Ghana. 
    </p>
</main>

<script src="page.js"></script>

<footer class="footer">
    <p>Haku Altanpurev and Indigo Clark, 2025.</p>
</footer>
</body>
</html>
